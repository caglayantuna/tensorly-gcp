{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to use custom loss and gradient function\nOn this page, you will find examples showing how to use your own loss with Generalized CP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nTensorly-Gcp allows you to make use of your own loss within GCP. Under the\nhood, GCP relies on `scipy.minimize.lbfgs` which requires a vector (1D) input.\nTherefore your custom loss and gradient functions must take a vector input.\nTo write the loss at the tensor level more easily, we provide some reshaping functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tlgcp import generalized_parafac\nfrom tensorly.metrics import RMSE\nimport numpy as np\nimport tensorly as tl\nimport time\nfrom tlgcp.utils import loss_operator\nfrom tlgcp.generalized_parafac._generalized_parafac import vectorized_factors_to_tensor, vectorized_mttkrp\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In GCP, we vectorize all the factors, however estimated tensor should be\nreconstructed inside the loss and gradient. Therefore, you should use\nvectorized_factors_to_tensor(x, shape, rank) to define estimated tensor\nin your function. Also, vectorized_mttkrp(gradient, x , rank) is necessary\nin your gradient function to follow GCP formulization, it simply computes\nand unfolds the MTTKRP. Knowing this, you can define your callable function,\nhere e.g. as fun_loss and fun_gradient.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fun_loss = lambda x: tl.sum((tensor - vectorized_factors_to_tensor(x, shape, rank)) ** 2) / size\nfun_gradient = lambda x: vectorized_mttkrp(2 * (vectorized_factors_to_tensor(x, shape, rank) - tensor), x, rank) / size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In these functions, x represents vectorized factors. In our loss functions,\nwe use the size (total number of entries) of the tensor for normalization as\nit is suggested in Hong, Kolda and Duersch's paper, but it is not mandatory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rank = 5\nshape = [60, 80, 50]\ncp_tensor = tl.cp_to_tensor(tl.random.random_cp(shape, rank))\narray = np.random.binomial(1, cp_tensor / (cp_tensor + 1), size=shape)\ntensor = tl.tensor(array, dtype='float')\nsize = tl.prod(tl.shape(tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we create a tensor and define necessary variables, we can apply GCP\nwith our defined loss and gradient functions. It should be noted that, loss\nshould be None to be able to use your functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tic = time.time()\ntensor_gcp, errors_gcp = generalized_parafac(tensor, rank=rank, return_errors=True,\n                                             loss=None, fun_loss=fun_loss, fun_gradient=fun_gradient)\ncp_reconstruction_gcp = tl.cp_to_tensor(tensor_gcp)\ntime_gcp = time.time() - tic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you may have noticed, the custom loss we used here is the Gaussian loss\nand its associated gradient. We can compare the results with the built-in\nGaussian loss inside Tensorly-gcp.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = \"gaussian\"\ntic = time.time()\ntensor_gcp_gaussian, errors_gcp_gaussian = generalized_parafac(tensor, rank=rank,return_errors=True,\n                                                               loss=loss)\ncp_reconstruction_gcp_gaussian = tl.cp_to_tensor(tensor_gcp)\ntime_gcp_gaussian = time.time() - tic\n\nprint(\"RMSE for GCP:\", RMSE(cp_tensor, cp_reconstruction_gcp))\nprint(\"RMSE for GCP:\", RMSE(cp_tensor, cp_reconstruction_gcp_gaussian))\n\nprint(\"Loss for GCP:\", tl.sum(loss_operator(cp_tensor, cp_reconstruction_gcp, loss)))\nprint(\"Loss for GCP:\", tl.sum(loss_operator(cp_tensor, cp_reconstruction_gcp_gaussian, loss)))\n\nprint(\"GCP time:\", time_gcp)\nprint(\"GCP time:\", time_gcp_gaussian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, results are very similar except processing time, the variation\nbeing due to random initialization. We can also observe this behaviour by\nplotting error per iteration for each method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def each_iteration(a, b):\n    fig=plt.figure()\n    fig.set_size_inches(10, fig.get_figheight(), forward=True)\n    plt.plot(a)\n    plt.plot(b)\n    plt.yscale('log')\n    plt.legend(['GCP-defined','GCP-gaussian'], loc='upper right')\n\n\neach_iteration(errors_gcp, errors_gcp_gaussian)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Generalized Parafac in Tensorly\nOn this page, you will find examples showing how to use Generalized CP (GCP).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nTensorly includes generalized decomposition [1] and its stochastic version (SGCP) [2]\nwhich allow using different kind of losses rather than only Euclidean. Both algorithms\naim at decreasing loss between the input tensor and estimated tensor by using\ngradient which is calculated according to the selected loss by the user.\nWhile GCP implementation uses FISTA method for optimization, SGCP uses ADAM\nalgorithm as in [2].\nFollowing table gives existing losses in Tensorly with their gradients and constraints:\n\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Distribution   | Loss                                          | Gradient                                          |Constraints           |\n|                |                                               |                                                   |                      |\n+================+===============================================+===================================================+======================+\n| Rayleigh       | $2\\log(m) + (\\pi/4)(x/(m + \\epsilon))^2$| $2\\times(m) + (\\pi/4)(x/(m))^2$             |$x>0, m>0$      |\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Bernoulli odds | $log(m + 1) - xlog(m + \\epsilon)$       | $1 / (m + 1) - x/(m + \\epsilon)$            |$x\\in(0,1), m>0$|\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Bernoulli logit| $log(1 + e^m) - xm$                     | $e^m / (e^m+1) - x$                         |$x\\in(0,1), m>0$|\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Gamma          | $x / (m + \\epsilon) + log(m + \\epsilon)$| $-x / ((m + \\epsilon)^2) + 1/(m + \\epsilon)$|$x>0, m>0$      |\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Poisson count  | $m - xlog(m + \\epsilon)$                | $1 - x/(m + \\epsilon)$                      |$m>0$           |\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Poisson log    | $e^m - xm$                              | $e^m - x$                                   |                      |\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n| Gaussian       | $(x - m)^2$                             | $2\\times(m - x)$                            |                      |\n+----------------+-----------------------------------------------+---------------------------------------------------+----------------------+\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tensorly as tl\nfrom tensorly.decomposition import non_negative_parafac_hals\nfrom tlgcp import generalized_parafac, stochastic_generalized_parafac\nfrom tensorly.metrics import RMSE\nfrom tlgcp.utils import loss_operator\nimport time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example with Bernoulli loss\nTo use GCP decomposition efficiently, loss should be selected according to the input tensor.\nHere, we will report an example with Bernoulli odds loss. Let us note that\nwe suggest to use random init rather than svd while using GCP decomposition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Parameters\ninit = 'random'\nrank = 5\nloss = 'bernoulli_odds'\nshape = [60, 80, 50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create a synthetic tensor wih Bernoulli distribution, we use random cp and numpy\nbinomial functions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cp_tensor = tl.cp_to_tensor(tl.random.random_cp(shape, rank))\narray = np.random.binomial(1, cp_tensor / (cp_tensor + 1), size=shape)\ntensor = tl.tensor(array, dtype='float')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GCP decomposition function requires loss as differ from\nexisting tensorly decomposition functions. It should be noted that loss\ncan be defined by the user.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# GCP\ntic = time.time()\ntensor_gcp, errors_gcp = generalized_parafac(tensor, rank=rank, init=init, return_errors=True, loss=loss, n_iter_max=500)\ncp_reconstruction_gcp = tl.cp_to_tensor(tensor_gcp)\ntime_gcp = time.time() - tic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stochastic GCP (SGCP) decomposition function requires learning rate (LR),\nbatch size, epochs and beta parameters (for ADAM) as input in addition to GCP\ndecomposition inputs. Fortunately, and beta parameters could be fixed thanks\nto the literature who works with ADAM optimization. Besides, in case of\nbadly chosen LR, SGCP updates the LR by dividing LR by 10 after each failed\niteration until reaching 20 successive bad iteration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SGCP\ntic = time.time()\ntensor_sgcp, errors_sgcp = stochastic_generalized_parafac(tensor, rank=rank, init=init,\n                                                          return_errors=True, loss=loss, lr=1e-3,\n                                                          n_iter_max=1000, batch_size=50, epochs=100)\ncp_reconstruction_sgcp = tl.cp_to_tensor(tensor_sgcp)\ntime_sgcp = time.time() - tic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compare GCP decompositions, we choose non-negative CP with HALS (NN-CP)\nsince Bernoulli odds has a non-negative constraint.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# NN-Parafac with HALS result\ntic = time.time()\ntensor_cp, errors = non_negative_parafac_hals(tensor, rank=rank, n_iter_max=100, init=init, return_errors=True)\ncp_reconstruction = tl.cp_to_tensor((tensor_cp))\ntime_cp = time.time() - tic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the example, we use binary tensor `tensor` as an input. It is possible to\nhave binary result by using numpy binomial function on reconstructed cp tensors.\nBesides, we could compare the results with initial `cp_tensor` and reconstructed tensors\nwithout calculating it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"RMSE for GCP:\", \"%.2f\" % RMSE(cp_tensor, cp_reconstruction_gcp))\nprint(\"RMSE for SGCP:\", \"%.2f\" %RMSE(cp_tensor, cp_reconstruction_sgcp))\nprint(\"RMSE for NN-CP:\", \"%.2f\" %RMSE(cp_tensor, cp_reconstruction))\n\nprint(\"Loss for GCP:\", \"%.2f\" %tl.sum(loss_operator(cp_tensor, cp_reconstruction_gcp, loss)))\nprint(\"Loss for SGCP:\", \"%.2f\" %tl.sum(loss_operator(cp_tensor, cp_reconstruction_sgcp, loss)))\nprint(\"Loss for NN-CP:\", \"%.2f\" %tl.sum(loss_operator(cp_tensor, cp_reconstruction, loss)))\n\nprint(\"GCP time:\", \"%.2f\" %time_gcp)\nprint(\"SGCP time:\", \"%.2f\" %time_sgcp)\nprint(\"NN-CP time:\", \"%.2f\" %time_cp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compare the results according to processing time, root mean square error and\nthe selected loss. According to the final Bernoulli loss,\nboth GCP and SGCP give better results than NN-CP. Since SGCP requires many\niteration inside each epoch, processing time is much more than the others.\nOn the other hand, NN-CP is better in terms of root mean square error as it is\nexpected.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n[1] Hong, D., Kolda, T. G., & Duersch, J. A. (2020).\nGeneralized canonical polyadic tensor decomposition.\nSIAM Review, 62(1), 133-163.\n`(Online version)\n<https://arxiv.org/abs/1808.07452>`_\n\n[2] Kolda, T. G., & Hong, D. (2020). Stochastic gradients for\nlarge-scale tensor decomposition.\nSIAM Journal on Mathematics of Data Science, 2(4), 1066-1095.\n`(Online version)\n<https://arxiv.org/abs/1906.01687>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
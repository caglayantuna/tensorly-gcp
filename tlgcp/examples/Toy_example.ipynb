{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import non_negative_parafac_hals\n",
    "from tlgcp import generalized_parafac, stochastic_generalized_parafac\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorly.metrics import RMSE\n",
    "from tlgcp.utils import loss_operator\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with Bernoulli loss\n",
    "# --------------------------------------------\n",
    "To use GCP decomposition efficiently, loss should be selected according to the input tensor.\n",
    "Here, we will report an example with Bernoulli odds loss. Let us note that\n",
    "we suggest to use random init rather than svd while using GCP decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "init = 'random'\n",
    "rank = 5\n",
    "loss = 'bernoulli_odds'\n",
    "shape = [60, 80, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a synthetic tensor wih Bernoulli distribution, we use random cp and numpy\n",
    "binomial functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_tensor = tl.cp_to_tensor(tl.random.random_cp(shape, rank))\n",
    "array = np.random.binomial(1, cp_tensor / (cp_tensor + 1), size=shape)\n",
    "tensor = tl.tensor(array, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "tensor_gcp, errors_gcp = generalized_parafac(tensor, rank=rank, init=init, return_errors=True, loss=loss,\n",
    "                                             n_iter_max=100)\n",
    "cp_reconstruction_gcp = tl.cp_to_tensor((tensor_gcp))\n",
    "time_gcp = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic GCP (SGCP) decomposition function requires batch size, epochs and beta\n",
    "parameters (for ADAM) as input in addition to GCP decomposition inputs. Fortunately,\n",
    "LR and beta parameters could be fixed thanks to the literature who works with\n",
    "ADAM optimization. Besides, in case of badly chosen LR, SGCP updates the LR by dividing\n",
    "LR by 10 after each failed iteration until reaching 20 successive bad iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sufficient number of bad epochs\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "tensor_sgcp, errors_sgcp = stochastic_generalized_parafac(tensor, rank=rank, init=init,\n",
    "                                                          return_errors=True, loss=loss, lr=1e-3,\n",
    "                                                          n_iter_max=1000, batch_size=50, epochs=100)\n",
    "cp_reconstruction_sgcp = tl.cp_to_tensor((tensor_sgcp))\n",
    "time_sgcp = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare GCP decompositions, we choose non-negative CP with HALS (NN-CP)\n",
    "since Bernoulli odds has a non-negative constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "tensor_cp, errors = non_negative_parafac_hals(tensor, rank=rank, n_iter_max=100, init=init, return_errors=True)\n",
    "cp_reconstruction = tl.cp_to_tensor((tensor_cp))\n",
    "time_cp = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we use binary tensor `tensor` as an input. It is possible to\n",
    "have binary result by using numpy binomial function on reconstructed cp tensors.\n",
    "Besides, we could compare the results with initial `cp_tensor` and reconstructed tensors\n",
    "without calculating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for GCP: 0.09052526945009017\n",
      "RMSE for SGCP: 0.12855131736168407\n",
      "RMSE for NN-CP: 0.3244237927372667\n",
      "Loss for GCP: 0.691108345545192\n",
      "Loss for SGCP: 0.7041216478854444\n",
      "Loss for NN-CP: 0.8731631402108244\n",
      "GCP time: 1.5200185775756836\n",
      "SGCP time: 13.93760895729065\n",
      "NN-CP time: 0.42238306999206543\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for GCP:\", RMSE(cp_tensor, cp_reconstruction_gcp))\n",
    "print(\"RMSE for SGCP:\", RMSE(cp_tensor, cp_reconstruction_sgcp))\n",
    "print(\"RMSE for NN-CP:\", RMSE(cp_tensor, cp_reconstruction))\n",
    "\n",
    "print(\"Loss for GCP:\", tl.sum(loss_operator(cp_tensor, cp_reconstruction_gcp, loss)))\n",
    "print(\"Loss for SGCP:\", tl.sum(loss_operator(cp_tensor, cp_reconstruction_sgcp, loss)))\n",
    "print(\"Loss for NN-CP:\", tl.sum(loss_operator(cp_tensor, cp_reconstruction, loss)))\n",
    "\n",
    "print(\"GCP time:\", time_gcp)\n",
    "print(\"SGCP time:\", time_sgcp)\n",
    "print(\"NN-CP time:\", time_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results according to processing time, root mean square error and\n",
    "the selected loss. According to the final Bernoulli loss,\n",
    "both GCP and SGCP give better results than NN-CP. Since SGCP requires many\n",
    "iteration inside each epoch, processing time is much more than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "----------\n",
    "\n",
    " [1] Hong, D., Kolda, T. G., & Duersch, J. A. (2020).\n",
    " Generalized canonical polyadic tensor decomposition.\n",
    " SIAM Review, 62(1), 133-163.\n",
    " `(Online version)\n",
    " <https://arxiv.org/abs/1808.07452>`_\n",
    "\n",
    " [2] Kolda, T. G., & Hong, D. (2020). Stochastic gradients for\n",
    " large-scale tensor decomposition.\n",
    " SIAM Journal on Mathematics of Data Science, 2(4), 1066-1095. `(Online version)\n",
    " <https://arxiv.org/abs/1906.01687>`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
